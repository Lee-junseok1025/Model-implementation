# Model Implementation
Pytroch Model Implmentation

| Model Name  |Paper|
| ------------- | ------------- |
| Coordinate Attention (CA) CA-CNN  | J. Cui, Q. Zhong, S. Zheng, L. Peng, and J. Wen, “A Lightweight Model for Bearing Fault Diagnosis Based on Gramian Angular Field and Coordinate Attention” Machines 2022, 10(4), 282.  |
| Self-attention Network (SANet)  | H. Fang et al., “You can get smaller: A lightweight self-activation convolution unit modified by transformer for fault diagnosis,” Adv. Eng. Informat., vol. 55, Jan. 2023, Art. no. 101890.|
| Lightwieght Convolution-based Transformer (LiConvFormer)  | S. Yan, et al. “LiConvFormer: A lightweight fault diagnosis framework using separable multiscale convolution and broadcast self-attention,” Expert Syst. Appl., vol. 237, Mar. 2024.  |
| squeeze-and-excitation (SE) Inverted Residual CNN (SE-IRCNN)  | D. He, et al. “A rolling bearing fault diagnosis method using novel lightweight neural network” Meas. Sci. Technol. 32 125102, 2021.  |
| Stacked Inverted Residual Convolution Neural Network (SIRCNN)  | D. Yao, et al. “A lightweight neural network with strong robustness for bearing fault diagnosis,” Meas., Volume 159, 2020  | 
| LiteFormer  | W. Sun, et al. "LiteFormer: A Lightweight and Efficient Transformer for Rotating Machine Fault Diagnosis," IEEE Trans. Reli., early access, 2023.  |
| LFEE-Net  | H. Fang, et al. "LEFE-Net: A Lightweight Efficient Feature Extraction Network with Strong Robustness for Bearing Fault Diagnosis," IEEE Trans. Instrum. Meas, vol. 70, pp. 1-11, 2021.  |
| MCDS-CNN | L. Ling, et al. “A lightweight bearing fault diagnosis method based on multi-channel depthwise separable convolutional neural network,” Electro., vol. 11, 2022.
| MIMTNet | Y. Wang, et al. "A Multi-Input and Multi-Task Convolutional Neural Network for Fault Diagnosis Based on Bearing Vibration Signal," IEEE Sensors Journal, vol. 21, no. 9, pp. 10946-10956, 1 May 2021.
| MANIQA | S. Yang et al., “Maniqa: Multi-dimension attention network for noreference image quality assessment,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) Workshops, Jun. 2022, pp. 1191–1200.
| ViT | A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for image recognition at scale,” 2020, arXiv:2010.11929.
| EfficientNetV2 | M. Tan et al., “EfficientNetV2: Smaller models and faster training,” in Proc. Int. Conf. Mach. Learn. (ICML), 2021, pp. 10096–10106.
| Differential Transformer | T. Ye et al. , "Differential Transformer" 2024. arXiv:2410.05258
